{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1nzC-FjL5NtoUu-G2pkj9M8r7E79thK4R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /kaggle/working/Comys_Hackathon5.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIAMESE NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#Seeding made fixed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "#Cuda declaration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#distortion folder flattening \n",
    "def flatten_distortion_folders(root_dir):\n",
    "    for class_name in os.listdir(root_dir):\n",
    "        class_path = os.path.join(root_dir, class_name)\n",
    "        distortion_path = os.path.join(class_path, \"distortion\")\n",
    "        if os.path.isdir(distortion_path):\n",
    "            for img_file in os.listdir(distortion_path):\n",
    "                src = os.path.join(distortion_path, img_file)\n",
    "                dst = os.path.join(class_path, img_file)\n",
    "                if os.path.isfile(src):\n",
    "                    shutil.move(src, dst)\n",
    "            os.rmdir(distortion_path)\n",
    "\n",
    "# Siamese Dataset\n",
    "class SiameseDistortionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = {\n",
    "            cls: sorted([\n",
    "                os.path.join(root_dir, cls, img)\n",
    "                for img in os.listdir(os.path.join(root_dir, cls))\n",
    "                if os.path.isfile(os.path.join(root_dir, cls, img))\n",
    "            ]) for cls in self.classes\n",
    "        }\n",
    "\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        for cls in self.classes:\n",
    "            same_class_imgs = self.image_paths[cls]\n",
    "            for i in range(len(same_class_imgs) - 1):\n",
    "                self.pairs.append((same_class_imgs[i], same_class_imgs[i + 1]))\n",
    "                self.labels.append(1)\n",
    "                other_classes = [c for c in self.classes if c != cls]\n",
    "                random.shuffle(other_classes)\n",
    "                for diff_cls in other_classes:\n",
    "                    if self.image_paths[diff_cls]:\n",
    "                        diff_img = self.image_paths[diff_cls][i % len(self.image_paths[diff_cls])]\n",
    "                        self.pairs.append((same_class_imgs[i], diff_img))\n",
    "                        self.labels.append(0)\n",
    "                        break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.pairs[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        img1 = np.array(Image.open(img1_path).convert('RGB'))\n",
    "        img2 = np.array(Image.open(img2_path).convert('RGB'))\n",
    "        if self.transform:\n",
    "            img1 = self.transform(image=img1)['image']\n",
    "            img2 = self.transform(image=img2)['image']\n",
    "        return img1, img2, label\n",
    "\n",
    "# Dataset for the Siamese model\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('convnext_atto', pretrained=True, num_classes=0)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.encoder.num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = self.projection(feat)\n",
    "        feat = F.normalize(feat, p=2, dim=1)\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.forward_once(x1), self.forward_once(x2)\n",
    "\n",
    "# Loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, feat1, feat2, label):\n",
    "        distance = F.pairwise_distance(feat1, feat2)\n",
    "        loss = label * distance.pow(2) + (1 - label) * (torch.clamp(self.margin - distance, min=0.0).pow(2))\n",
    "        return loss.mean()\n",
    "\n",
    "# Calculating metrics\n",
    "def compute_all_metrics(preds, labels):\n",
    "    preds_np = np.array(preds)\n",
    "    labels_np = np.array(labels)\n",
    "    acc = (preds_np == labels_np).mean()\n",
    "    precision = precision_score(labels_np, preds_np, zero_division=0)\n",
    "    recall = recall_score(labels_np, preds_np, zero_division=0)\n",
    "    f1 = f1_score(labels_np, preds_np, zero_division=0)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Loading data\n",
    "train = \"/kaggle/working/Comys_Hackathon5/Task_B/train\"\n",
    "val = \"/kaggle/working/Comys_Hackathon5/Task_B/val\"\n",
    "flatten_distortion_folders(train)\n",
    "flatten_distortion_folders(val)\n",
    "\n",
    "# transforms\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Train and Val loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_dataset = SiameseDistortionDataset(root_dir=train, transform=train_transform)\n",
    "val_dataset = SiameseDistortionDataset(root_dir=val, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, generator=g, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Training loop\n",
    "model = SiameseNet().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_acc = 0\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    all_train_preds, all_train_labels = [], []\n",
    "    total_train_loss = 0\n",
    "    for img1, img2, label in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/20\"):\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            feat1, feat2 = model(img1, img2)\n",
    "            loss = criterion(feat1, feat2, label)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            distance = F.pairwise_distance(feat1, feat2)\n",
    "            preds = (distance < 0.5).float()\n",
    "            all_train_preds.extend(preds.cpu().numpy())\n",
    "            all_train_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    train_acc, train_prec, train_recall, train_f1 = compute_all_metrics(all_train_preds, all_train_labels)\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "#validation loop\n",
    "    model.eval()\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in tqdm(val_loader, desc=f\"[Val] Epoch {epoch+1}/20\"):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            feat1, feat2 = model(img1, img2)\n",
    "            loss = criterion(feat1, feat2, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            distance = F.pairwise_distance(feat1, feat2)\n",
    "            preds = (distance < 0.5).float()\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    val_acc, val_prec, val_recall, val_f1 = compute_all_metrics(all_val_preds, all_val_labels)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, Precision={train_prec:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}, Precision={val_prec:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}\\n\")\n",
    "\n",
    "#Saving best model and early stopping\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_siamese_convnext.pt')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
